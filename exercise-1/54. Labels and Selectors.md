We just added a DaemonSet for the random number generator

and then we saw our performance improve.

So, we're assuming that that meant the service that's

sending the connections to rng, but that service somehow

can see both the DaemonSet pod,

as well as the original deployment pod, for rng,

because we haven't gotten rid of the old one.

But, now we have these two pods, but they're in different

resources and the same service

is sending connections to both.

Why is that happening?

This all comes down to labels and selectors.

We've talked a little bit about labels before, but

we haven't talked about selectors.

Selectors are the glue that connect

resources to each other.

When you create a deployment, for example,

a deployment needs to create a ReplicaSet.

It uses a selector to find the

labels of the things it needs to manage.

The same is true of that ReplicaSet finding its pods.

It turns out in this case, that's exactly what's happening.

If we go and use a kubectl describe service rng, describe

service rng.

We'll get some information back, including the selector.

The selector is how it finds

the pods it's going to send connections to.

It's that easy. In this case, it's only

one, key value.

It's the app=rng.

If any pod has app=rng,

it will consider that a part of the pool of pods

to round robin these connections to.

This is how other resources work as well.

Other resources, like deployments that create ReplicaSets,

they use that same selector

to determine which ReplicaSets they manage or

they own. This case, we could have

multiple labels, but the only one

that matters for this resource, the

rng service.

The only one that matters is this selector of app=rng.

There can be other labels assigned to pods,

but it's only going to care about this one.

It's just looking for this. If it matches, it's a part of

the group.

We can use selectors from the command line when we're

asking for resources, like when we use the get command, or

even the log command, and other commands, we can use

a selector to filter our view or to

return certain sets of resources based

on certain labels that we want to get.

That's kind of similar to this because this is

essentially a resource asking for

what resources it should manage by going and looking

for a label, or a set of labels.

We can explore this, as well, from the command line to see

what we would see if we were the service with this

type of selector.

If we do a kubectl get pods, and

we filter it, or use the selector

app=rng, we get

back two pods.

Again, these two pods were created by other resources.

Different resources.

One by the DaemonSet.

So, those two are still filterable,

or selectable, by this

particular label key value.

So, it's not really any fancier than that.

In fact, it's so simple.

It's easy to understand once you grasp the idea that

it's simply like doing a...it's like doing a SQL

query, basically. It's doing a simple query

of all resources of a certain type.

That's the key is the other part of this, is it has to

match the type. So, a service is always looking for

pods to send connections to.

It's not going to try to send connections to other

services or whatever.

It's going to focus on pods.

It's just going to simply do, in this case, that

key value app rng, and if it matches,

it sends a connection. But, why is that happening in this

case? I mean, how did the DaemonSet get the

app=rng?

Well, this has to do with how these labels are created

to begin with. Some of the commands we've been running

have been auto creating labels for us as a convenience.

When you create a deployment, and we type this first

command kubectl create deployment rng.

When we type that, it

has to assign a label because that's how it knows

which pods are it's, right.

That's how it assigns the pods is it creates those pods

with a template that gives those a label of app=rng.

That's automatic. We don't have to tell that deployment.

That's part of the deployment's generator.

If you remember us talking about generators, that's

part of that. It's generating this template and

assigning the app=rng, and it gets that name

from the name of the deployment itself.

If you go and look at the labels, then you'll see that

those have all a label of app=rng.

It continues all the way down.

So, the deployment itself has the app rng, then the

ReplicaSet that it creates will have the app rng.

Then the pods will also have the app rng.

That's how they link to each other.

If you think of these as sort of links in the database,

that's kind of how this all works.

The problem for us was when we took that file from

the deployment, edited the YAML file and then

deployed it as a DaemonSet, we kind

of forgot to take out that label.

Of course, we didn't know that. This is one of the reasons

why in the real world, you don't want to be taking one

type of YAML from one resource and repurposing it for

another without really knowing what you're doing.

Because we accidentally reused

that same label, left it in there.

So, when the DaemonSet created it, it

also had that label.

You'll notice that other types of commands have slightly

different labels. If you use a run command, for example, to

create a deployment, it will add run

equals whatever the name is you gave it, rather

than the app. The app is something straight from the create

deployment command.

Let's see if we can't go back into these resources

and fix it so that they're not a part of the same

service.

