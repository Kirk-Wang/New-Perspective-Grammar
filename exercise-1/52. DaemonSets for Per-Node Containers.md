You might call these DemonSets.

你可以称这些为 DemonSets。

You might call them DaemonSets.

你可以称它们为 DemonSets。

Either way is acceptable with me.

无论哪种方式我都能接受。
* acceptable adj. 可接受的；合意的；可忍受的

This is a new type of resource that we haven't used yet.

这是一种我们还没有使用过的新型资源。

It is not a deployment.

这不是部署。

It does not use ReplicaSets.

它不使用复制集。

It still uses pods like any pod type

of resource. That main purpose is to run

the same image on each node.

它仍然像任何 pod 资源一样使用 pod。
主要目的是在每个节点上运行相同的 image。

We can do things like filter, and we'll talk a little bit

about that where it's not on all nodes, but the idea is one

of these per node.

我们可以做一些像过滤的事情，我们会讨论一下它不是在所有的节点上，但思想是每个节点上有一个。

Where we could use deployments, but in deployments,

在我们可以使用部署的地方，但是在部署中，它不是完全相同的方式。

it's not quite the same way. We could spin this up and say

10 deployments of the rng versus 10 nodes

with a DaemonSet, but it doesn't really get us the same

我们可以把它转起来，比如说10个 rng 部署，
而不是10个带有守护进程的节点，但是它并没有得到相同的结果。

result. We could try this command with a scale deployment

rng, but it doesn't guarantee that the rng

will be evenly distributed, that there'll be one on each

我们可以使用 scale deployment rng 来尝试这个命令，
但它不能保证 rng 将均匀分布，每个节点上都有一个。

node. Remember, our imaginary

conclusion is that maybe the rng is running

out of entropy, that that's some sort of finite resource

记住，我们假想得出的结论是，rng 可能已经耗尽了熵，这是每台服务器的某种有限资源。

per server. We need to put it across a bunch of different

nodes so that we're getting different sets of entropy

and different OS's.

我们需要把它放在一堆不同的节点上，这样我们就能得到不同的熵集和不同的操作系统。

That's our theory.

这是我们的理论。

We need to prove that theory, but we need to do

a DaemonSet to ensure that each node

gets one of these.

我们需要证明这个理论，
但是我们需要执行一个守护进程，
以确保每个节点都能获得其中一个。

Another benefit here is if we add nodes or take them away,

when we add them, they will automatically get

another one of the DaemonSets.

这里的另一个好处是，如果我们添加或带走节点，
当我们添加它们时，它们将自动获得另一个守护进程。

If we take a node away, that pod

won't get redeployed somewhere else.

如果我们拿走一个节点，pod 就不会被重新部署到其他地方。

It just won't happen.

这是不可能的。

It ensures that we're not loading up too many on one

node.

它确保我们不会在一个节点上加载太多的内容。

It's a different way of doing things.

这是一种不同的做事方式。

But, what would you use this for in the real world, you

might ask? When it's not an imaginary scenario about

random numbers on Linux?

Well, there's lots of good reasons.

Kube-proxy is already doing that.

If you start adding more nodes, kube-proxy will

automatically show up on that node.

Why is that? It could be the DaemonSet, depending on your

deployment option. You can use CNI plugins which

will also set up these as well.

You can go into the kube-system namespace and look

around. You might find that you already have some of these

running. You just didn't know it.

Monitoring agents, backup agents, security tools,

all these sorts of things are common for necessarily

running on each node, instead of three or four of

them running on a single node.

That's what we can guarantee with the DaemonSet resource.

The lastm little thing I will note here is you're probably

thinking, well, what if I only want the DaemonSet on some

nodes? You can use labels and selectors,

and that allows you to filter which nodes will be able to

run those. We'll talk about that later when we get

into that topic.

You might wonder why we talked about the YAML,

and then we brought up the Dashboard before we did this

topic. Well, that's because we don't have a

command line for the DaemonSet. I mentioned that a little

bit earlier.

Not every resource type, in fact, if you look at the list,

most resources can't be created with

kubectl create commands. Those are really only there for

the most common types of resources.

So, what we can do, though, is use the apply command.

The apply command will simply create, or change,

resources based on the YAML, and every resource

type can be put into YAML.

You might be wondering how we create this YAML to start

with. Obviously, we could go and we could

read the documentation.

We could spend hours doing that.

That's honestly what I did when I didn't know what I was

doing. I didn't know that I could take it from a blog,

or take it from the website for the Kubernetes docs.

Then I had to mess around with that.

We could potentially get some

YAML from what stuff we already got.

That's coming up next.

