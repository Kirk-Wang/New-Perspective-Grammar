If you're used to thinking about accessing containers

the way that Docker thinks of it, get ready to rewire

your brain. Because it's a little bit different in

Kubernetes and we have, of course, way more options.

To start off with, when you've created these pods so

far, there's nothing stopping the pods

from talking to each other. We'll actually talk about the

networking underlying systems, and CNI, and all of that in

a little bit. Until then, just know that pods, right

now, can still talk to each other if we were trying to talk

directly to IP addresses.

But, what we really want is a stable DNS address

for those pods that we need to talk to.

Maybe you're having an API and you want to put a web

interface in front of that that talks to the API, so it

needs a reliable DNS name to find that

API set of pods.

Maybe it's one pod, maybe it's five replicas of a pod.

It doesn't really matter.

The point is, the resource we need to make here is

known as services.

We expose services

by creating these resources that then point

to backend pods.

We're going to use the kubectl expose command to

do this. This is one of the many ways that you can create

this resource.

There's multiple service types.

In fact, out-of-the-box you get at least a few.

Depending on where you're installing Kubernetes and

what add-ons you have, you might have more than a few

options for these services.

Remember that CoreDNS we talked about, that's part of

the control plane? Well, this is the first job that that

CoreDNS is going to do.

It's going to provide the DNS resolution for

these services so that other pods in our cluster

can access these pods through its service name.

The first two types of these services

everybody gets. We have ClusterIP and NodePort.

These are the two that work in every Kubernetes setup.

You can imagine, under the hood, a lot of this is

controlled by kube-proxy.

The API, of course, is providing the endpoint for us

to configure this. Obviously, these rules are stored

in etcd, but it's kube-proxy out-of-the-box

that's going to control this using largely IP

tables, which is a Linux subsystem that

is a part of the kernel for controlling routing of

packets and firewall stuff.

If you're a Linux guru, you already know all about IP

tables, and I don't need to go down that road with you.

But, the vast majority of us nowadays

can largely ignore IP tables, especially in

containers, because that's what Docker and Kubernetes are

designed to do. They're allowing those of us that just need

to use this to deploy apps to not have to be experts

in the underlying technology in the kernel on how packets

move around. But we're going to, of course, draw a few

diagrams here to just make sure we've got the concepts

down. The first one up is ClusterIP.

It's the default when we use the expose command.

If you don't specify a type of service, it will create a

cluster IP. The big deal about cluster IP's

are that they're only available in the cluster.

Which means if you're putting them in front of a web

server, the only other pods that will be able to access

it are going to be in that cluster.

It's not good for exposing to the rest of the internet

or something outside the cluster.

That's for NodePort and other types we'll get to later.

The nice thing with ClusterIP is that when we create it,

it'll use the port that our app is listening on.

So, we don't have to worry about some high port that we'd

have to memorize, or write down, or configure.

That's the limitation of NodePort.

So NodePort is designed to be accessible from

outside of your cluster.

It will pick a port from a really high range

that's free and available on every node, and it will listen

on all the nodes on that port.

Unfortunately, it's going to be a high number, so it's not

going to be able to use something like 80 out-of-the-box

with this expose command.

It's not always the right fit.

ClusterIP is, of course, since it's just inside your

container, a really easy one to use when you're dealing

with backend services. For stuff that you're going to put

publicly in front of your cluster

and accessed by someone else on the internet, or in your

data center, the most common here is to use

LoadBalancer.

LoadBalancer is using an external

third-party service. It might be an external proxy

or some sort of firewall service.

If it's in a data center or if it's in the cloud, it's

often going to be that cloud's external load balancer.

The cool thing about running Kubernetes, that's provisioned

automatically by the big cloud companies, is that they will

set up this load balancer service for you in your cluster

so that it automatically works when you ask for one.

The difference is, is that for those of us doing this on

our machine locally, a lot of us won't have this load

balancer option because it typically requires third-party

tooling. The cool thing today is that if you're running

Docker Desktop,

the load balancer is how you can get to use localhost

to expose services to your local machine.

We'll see that as well.

Let's back up a second and you'll notice the ClusterIP,

the NodePort, and then the load balancer.

Those are actually building on top of each other.

So, when you create a node port type of service,

remember that's the one that gives you a high port that's a

public IP port on all nodes.

That node port will then route traffic to a cluster

IP internally to the cluster to get to

your pods. If you choose to have a load balancer, again,

that's largely dependent upon third-party tooling and

services, then that will also create a node

port and a cluster IP to route that traffic

through. We'll see that in a second.

External name is something where you usually need

to have an internal DNS name inside your cluster,

or something outside your cluster, that you want to

be able to route to and you want to control that name

inside your cluster. It's a it's a pretty special case

thing. It's usually for when the name remotely

might change outside of your cluster, but you

don't want to have to redeploy your services just because

that external service might change.

So, you create this special DNS entry inside

your Kubernetes cluster, and then you can change that on

the fly. Kind of like a CNAME.

It won't require you to reprovision your

deployments and your container pods just because

some name on the internet changed.

That's a pretty rare one, I think, but it's still there

as a free option.