Let's get ready to run some containers!

All right. This is the first time we're going to create a

real container on our own.

Are you excited? Let's get started.

Remember, we can't create containers directly.

This is an important part that I want to keep reiterating.

That we, at the lowest level,

can only create a pod.

In most cases, you're not even going to create a pod.

You're going to create something higher than that in the

abstraction layers.

We're going to explain all that in this video.

Let's just start with a simple command.

We're going to use a kubectl run, which

is as close as you can get to a docker run command.

We're going to do that to start a simple Alpine image that

is going to overwrite the default cmd.

That's the command inside the Dockerfile, remember?

We're going to tell it, don't start a shell like

Alpine would do by default.

Just start the ping command and then ping a common, free

DNS service from Cloudflare

called 1.1.1.1. Lets type that at the command line now.

I'm going to name it pingpong.

Then, I have to put in an image.

That's required.

Then, this is optional, but we're going to override that

default command to ping

1.1.1.1. When we hit that, we do get a warning

about a deprecated feature.

It's explaining to us something that we'll talk about

later. Essentially, it's saying the command line for

kubectl run is changing, and its behavior will be different

in the future, and we'll pay attention

that later. For now, let's just note that last line.

It says this was created.

Great. We should be able to now do a

get all and see what we can see at the command line.

We're going to type a kubectl get all here

and we should see a new pod.

What's interesting here is that you see more than one

new object.

This again, remember, is getting us all the resources

in our cluster, and we knew that this one already existed.

Like we talked earlier, this is the API connection

point for anything in our cluster if they want to talk to

the API. We have these three new ones.

You'll notice, up at the top, we have a pod down

here in the middle. We have deployment.apps.

Then a replicaret.apps. Each one of these

has something with the name pingpong in it.

You'll notice that there's variations here in the

pod one and the replicaset.

What's interesting is we only ran one command,

but we got three objects out of it.

Why is that? That has to do with the layers

of abstractions inside the Kubernetes

object model. Maybe it's a little easier if we draw

this out. We don't know this yet, but

when we ran the kubectl run command,

it used something called a generator, which is a fancy way

of saying it created a specification in the background,

using a template, for us to create resource objects.

What it did for us was instead of creating a pod directly,

it created something called a deployment.

Let's just think of that as an abstraction.

It's not a real thing.

It's an object in the etcd database.

But, it's not a physical thing on disk.

It's simply a configuration that's known in

Kubernetes as a spec, or a specification, or

a resource. It turns out that deployments don't create

containers directly.

We know we have to create a pod at some point, but a

deployment doesn't actually create the pod.

It creates another resource object, called the ReplicaSet.

Then, that resource finally creates

the pod.

It's interesting to note here is that all three of

these resources are really what I call abstractions.

They're layers of different functionality split

up for different purposes and to allow us to have more

flexibility in how we use Kubernetes.

This is also a source of potential confusion and complexity

because there are so many of these resources here.

But, you'll see later how it gives us a lot more

flexibility as we go.

Finally, that pod creates the container, which is

a real object running in Docker.

Inside that container, it's running the ping command.

What we have here is a set of imaginary things inside

the object model. These resources.

When we type that run command, it first created a

deployment, which is a high-level construct that

allows for certain features inside it, known as the

deployment spec.

Then a part of that deployment spec is to create a

ReplicaSet.

That ReplicaSet resource then created

a pod spec.

There's specs all the way down.

We'll see this in YAML later.

It'll make a little more sense when you see that

conceptually in data.

The last thing to note here is if you see any older

documentation talking about a replication controller,

that's technically a deprecated feature before we

had ReplicaSets, and it was known as the replication

controller. Now we know the proper way to do things is to

create the deployment, and then that will create the

ReplicaSet, which will create the pods, which creates

the container.

That's why you see those different resources all

broken out when we do the get all command.

Each one of these is based on the

thing above it. In fact, when you see the names of them,

you'll notice that the deployment is called the pingpong

like we created it.

We created it with that name, pingpong.

Then the lower level objects have an extra,

random object ID on the end of them because

they're created from the upper spec.

So technically, if we were to update this deployment later

and it needed to replace the ReplicaSet, it

might end up with a different ID, depending on what we're

changing. Same is true of the pod.

Notice that the pod has different parts of the IDs here,

and that is one indicator of many

different indicators that means these are all coming from

something else. We didn't create a pod directly.

We first created the deployment, then the ReplicaSet in the

pod. Yes, I know this seems like a lot of extra

stuff in the way of just creating a container.

Again, Kubernetes is designed for you to create

real production systems.

It's not really designed for simplistic, single

container docker runs like stuff.

We already have Docker to do that.

Kubernetes wasn't there to solve that problem because it

had already been solved. It's trying to solve more complex

problems. Which is what we'll see later as deployments

will allow us to create rolling updates, without downtime,

and a whole lot of other features.

