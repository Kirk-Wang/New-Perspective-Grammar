Out-of-the-box, Kubernetes has something called kubenet,

which you can use. If you're doing something like

Docker Desktop or minikube, you might find that some

of these tools are using kubenet, which works great on a

single node. It doesn't provide a lot of features, but it

is there. What most of us will do in our real

world setups is use something called CNI plugins.

That's an abbreviation for the Container Network Interface,

which is a standard in the industry that allows

all of these third-party vendors to support Kubernetes

with their plugins.

Some of the jobs in your cluster are delegated

to these plugins, including things like getting an IP

address. That's what we call IPAM, IP Address

Management, and doing other things like creating

the network interface itself inside the pod.

The more you dive into the networking of Kubernetes, the

more you will see different parts of the puzzle

that are managed by different parts of the puzzle.

The first up here is the pod-to-pod communication,

which doesn't happen a whole lot because typically, our

pods are communicating through services.

So, there is an ability for you to talk to the pods

directly from the host, and vice versa.

But, that all tends to be managed by the CNI.

When we talk about our pods talking to a service,

like we just did in our httpenv example,

that's typically managed by kube-proxy, or if you

decide to use kube-router, kube-router.

Those parts act as the load

balancer in between your container and talking to that

service. This last one here is a biggie.

The big part about these CNI plugins is they may,

or may not, support something called network

policies.

That is a key part of your CNI choice.

When you choose your plugin, you have to decide, do I need

to protect certain pods from other

pods and their communications in this cluster?

The bigger your clusters get, and the more unrelated

services you start to run in that cluster, the more you're

likely going to want to segment your network

infrastructure inside the same cluster.

That's what network policies are for.

That feature is not available in every CNI.

So, you'll see some of the resources listed in these

lectures around networking and listing out the

CNI options.

There's websites that have different comparisons of them

and show you which ones are default for each deployment

option for your cluster.

There's lots of stuff out there to read.

We're not going to go through all that just yet because

that stuff changes rapidly.

New things come into the market.

Old products, or open source tools, fall off the list

because they're no longer supported.

Some get added new features and are now supporting

network policies when they previously didn't.

But, if you think now that you're going to need

at some point in this clusters future, the ability

to protect certain pods from other pods

by network firewall rules and the like, then

you're going to want a CNI with network policies

built in. Because if you're thinking about it right now,

changing the CNI in the middle of a cluster

deployment is a pretty unrealistic

thing to expect. Which means if you need to change your CNI

later, because maybe the one you have doesn't have the

features you want, you're going to need to build a new

cluster, and then you're going to need to move your stuff

over. Don't freak out at that, because one

of the core ideas here when you're making these clusters,

and we'll talk more about this throughout the course, is

that you need to design them in a way that they can be

replaced in the future.

You don't want to make these clusters so special and so

brittle that you simply can't replace it with a new one,

because you're going to have to make changes later.

You're going to have to make decisions that would

affect your entire cluster.

You may just decide, for safety reasons, to build a new

cluster and move your apps over.

You won't have to do that all the time, but one of the core

tenants here of DevOps is that we're not creating brittle

and fragile infrastructure that we can't replace

in a reasonable amount of time.

If you decide to get a little bit more complicated

than we've already talked about, you can do things

where certain parts of your infrastructure are doing

multiple parts of this networking component.

In other words, you can have certain connections coming in

through load balancers.

You can have other connections coming in through

kube-proxy. You can have other connections coming in

through a meta-plugin.

You can have other types of networking than just

the default CNI plugin that you choose.

You can have meta CNI plugins where you choose

multiples.

I've not seen that in the wild myself.

Most people are trying to keep their clusters as small and

as simple as possible.

It is an option once you get to that level of complexity.

