We're going to take some output YAML from one of our

existing resources and try to turn it into something that

will work for a DaemonSet.

Here we have a command for kubectl get that's going

to do an output of YAML format for

the existing deployment rng, and let's see if we

can't just edit that and turn it into what we need.

Output of YAML.

Then, we're going to save it to a file with the greater

than symbol rng.yml.

It doesn't really matter what you name it.

Let's open that up with our editor, whatever

your editor might be.

Let's take a look at it real quick.

If you look in that file, there is a second line there of

kind. That is the resource type.

What if we just change that resource type and

leave the rest the same? Let's try that.

Turn it

from deployment into DaemonSet. Let's

just save and quit that.

Then do a kubectl apply -f

with that file name.

All right. We got a bunch of errors back.

The reason that we're getting these errors is because

the command line does validation of the YAML

spec before it sends it to the server.

This is basically a step to save you time so that you're

not wasting potential errors in a server API, and it's

doing it locally against what it thinks is the proper

schema. If you look through here, there's several errors,

including ones about unknown field replicas.

Because that makes sense in a DaemonSet, we

don't have replicas. It's always going to be one per node.

There's no need to even have a concept of replicas.

So, it's telling us here that that doesn't belong in this

YAML spec.

You can see the same thing elsewhere.

It's got other errors that are very similar.

So, our first attempt didn't work so well.

It turns out that there is at least three or four different

things in this file that aren't supposed to be

there and that the API won't like.

What if we could use a force to just make it happen?

Well, not that kind of force, Luke, but more of a force

in the command line to ignore the spec and just

send it and basically say, trust me, I know what I'm doing.

Well, there is that. It's called

--validate=false. Normally, we have the command line

validating our YAML, and we're just saying ignore

the fact that it doesn't properly validate.

Just go ahead and send it to the API anyway.

Let's try it again.

kubectl apply -f rng.yml validate=false.

All right. That's a good sign.

It says it was created.

But, can it be that easy?

Is it really working like we expect it to?

Let's go dig around and see what we

can look at determine if it's doing what we expect.

We do a kubectl get all.

You notice we still have the orange-y deployment.

Now, we have this new thing under DaemonSet,

a brand new resource type that we haven't seen in the get

all list before.

If we were to scroll up and look at pods, we would notice

that there is one too many pods.

We have two different ones now.

One we just created. That's the second line.

The first one is from the deployment.

Remember, we can have different resource

types, with the same name, in the same

namespace, because we've already done that, right.

We have httpenv we created earlier.

We have services here where we create the deployment, and

then we create a service with the exact same name.

That's also true of a DaemonSet.

We can have a deployment with rng and a DaemonSet with rng.

It's okay as long as they're not the same name with

the same resource type.

You'll notice on this line down here, if you're not in a

multi-node setup like I am not.

I just have the single Docker Desktop locally.

You'll notice that you have a desired here.

So, this desired current ready.

These are all new features of the DaemonSet in the get

all kubectl, and that is going to match

the number of nodes you have.

So, if you have a two-node setup, you'd see two here.

If you had a three-node setup, you'd see three here.

The reason for that is that again, it's issuing

one pod per node regardless of how many you have.

Like we saw, we now have too many pods.

We have the ones from deployment and the DaemonSet.

You can tell the names a little bit because each

nested resource type is going to

create part of the name.

So that dash, the way that I know this and to sort

of a little bit inside information, that

the rng first part here is technically

the ReplicaSet that was created.

So, on the original deployment, the deployment

created a ReplicaSet, and the ReplicaSet created a pod, and

that's what created that full name.

Whereas, we don't have ReplicaSets inside

of a DaemonSet. We just need the pod name right

there. Just in case you have a multi-node

setup, or you're using something like kubeadm, or some

other server deployment solution,

it may taint your master nodes.

Those control plane nodes, even on a one-node setup,

depending on the tools you use, may prevent regular

pods from running on those master nodes.

That's also true of DaemonSet.

It will prevent potentially these

DaemonSets from running on the master node, which would

mean that you would have one less if you just had one

master. You'd have one less DaemonSet pod

running in the total count.

And that's normal to keep regular applications off of your

masters in a multi-node setup. We're, for

this little part of the course that we're just on a test

machine, we're using Docker Desktop, or MicroK8s,

or K3s, or MiniKube or all

these other options that are designed for learning, and

testing, and local developer setups.

Those don't typically have the taints, but even

if you had taints, it's ok.

We'll talk about taints and tolerations later.

Tolerations allow you to put pods on a master

even though they have a taint preventing it.

All right. Let's go to look at our webui and see if that

made any difference.

All right. On my machine, I was pretty steady at

10 for the miners.

But, the previous deployment, remember, we deployed

a YAML earlier, and that caused me to

have a pretty steady clip there.

Now, I'm up at 15. So, this definitely did something.

Just even adding one rng through the

DaemonSet, in addition to the deployment, made

a difference. But, I've got two containers now, and

if you're on a single-node setup like me, you've got just

those two containers. So, why did two suddenly

make a difference? If you think about it, we added a

DaemonSet, but the only way this could have possibly

improved is if the service

was using both of the pods.

But, if it's using both of the pods, that means that

somehow our DaemonSet, and our deployment, are

both under the same service.

This is kind of weird. In the next video, we're going to

talk about what just happened and why it may, or may

not, be a good thing for you.

