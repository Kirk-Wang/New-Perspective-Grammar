We're going to take some output YAML from one of our

existing resources and try to turn it into something that

will work for a DaemonSet.

我们将从我们现有的资源中获取一些输出 YAML，
并尝试将其转换为一些可用于 DaemonSet 的东西。

Here we have a command for kubectl get that's going

to do an output of YAML format for

the existing deployment rng, and let's see if we

can't just edit that and turn it into what we need.

这里我们有一个 kubectl get 命令，
它将为现有的部署 rng 执行 YAML 格式的输出，让
我们看看能否编辑它并将其转换为我们需要的内容。

Output of YAML.

YAML 的输出。

Then, we're going to save it to a file with the greater

than symbol rng.yml.

然后，我们将使用 `> rng.ym` 保存到一个文件。

It doesn't really matter what you name it.

命名并不重要。

Let's open that up with our editor, whatever

your editor might be.

无论您的编辑器是什么，让我们用编辑器打开它。

Let's take a look at it real quick.

让我们快速地看一下它。

If you look in that file, there is a second line there of

kind. That is the resource type.

如果你看一下那个文件，有第二行。这就是资源类型。

What if we just change that resource type and

leave the rest the same? Let's try that.

如果我们只是改变资源类型，而让其余的保持不变，会怎么样呢?让我们试试。

Turn it

from deployment into DaemonSet. Let's

just save and quit that.

将它从 deployment 变为 DaemonSet。让我们保存并退出它。

Then do a kubectl apply -f

with that file name.

然后使用该文件名执行 kubectl apply -f。

All right. We got a bunch of errors back.

好吧。我们得到了一堆错误。

The reason that we're getting these errors is because

the command line does validation of the YAML

spec before it sends it to the server.

我们得到这些错误的原因是命令行在将 YAML 规范发送到服务器之前进行了验证。

This is basically a step to save you time so that you're

not wasting potential errors in a server API, and it's

doing it locally against what it thinks is the proper

这基本上是一个节省时间的步骤，
这样您就不会浪费服务器 API 中潜在的错误，
并且它会根据它认为正确的模式在本地执行它。

schema. If you look through here, there's several errors,

including ones about unknown field replicas.

如果您仔细查看这里，会发现有几个错误，包括关于未知字段副本的错误。

Because that makes sense in a DaemonSet, we

don't have replicas. It's always going to be one per node.

因为这在 DaemonSet 中是有意义的，我们没有 replicas。它总是每个节点一个。

There's no need to even have a concept of replicas.

甚至不需要有副本的概念。

So, it's telling us here that that doesn't belong in this

YAML spec.

它告诉我们它不属于 YAML 规范。

You can see the same thing elsewhere.

你可以在其他地方看到同样的事情。

It's got other errors that are very similar.

它还有其他类似的错误。

So, our first attempt didn't work so well.

所以，我们的第一次尝试不太成功。

It turns out that there is at least three or four different

things in this file that aren't supposed to be

there and that the API won't like.

事实证明，这个文件中至少有三到四种 API 不喜欢的东西。

What if we could use a force to just make it happen?

如果我们可以用一种力量让它发生呢?

Well, not that kind of force, Luke, but more of a force

in the command line to ignore the spec and just

send it and basically say, trust me, I know what I'm doing.

嗯，不是那种力量，而是命令行中的一种力量，它忽略了规范，
只是发送它，基本上说，相信我，我知道我在做什么。

Well, there is that. It's called

--validate=false. Normally, we have the command line

validating our YAML, and we're just saying ignore

the fact that it doesn't properly validate.

嗯，就是这样。它被称为 --validate=false。
通常，我们有命令行验证我们的 YAML，
我们只是说忽略它没有正确验证的事实。

Just go ahead and send it to the API anyway.

不管怎样，只要把它发送到API就行了。

Let's try it again.

让我们再试一次。

kubectl apply -f rng.yml validate=false.

All right. That's a good sign.

好吧。这是个好兆头。

It says it was created.

它说它是被创造出来的。

But, can it be that easy?

但是，真的那么容易吗?

Is it really working like we expect it to?

它真的像我们期望的那样工作吗?

Let's go dig around and see what we

can look at determine if it's doing what we expect.

让我们去挖掘一下，看看我们能看到什么来确定它是否在做我们期望的事情。

We do a kubectl get all.

我们做一个 kubectl get all。

You notice we still have the orange-y deployment.

你注意到我们仍然有 orange-y 的部署。

Now, we have this new thing under DaemonSet,

a brand new resource type that we haven't seen in the get

all list before.

现在，我们在 DaemonSet 下有了这个新东西，
这是一种全新的资源类型，
我们以前从未在 get all 列表中看到过。

If we were to scroll up and look at pods, we would notice

that there is one too many pods.

如果我们向上滚动并查看 pods，我们会注意到有一个太多的 pods。

We have two different ones now.

我们现在有两个不同的。

One we just created. That's the second line.

一个我们刚刚创建的。这是第二行。

The first one is from the deployment.

第一个是 deployment。

Remember, we can have different resource

types, with the same name, in the same

namespace, because we've already done that, right.

记住，我们可以有不同的资源类型，
有相同的名字，在相同的命名空间里，因为我们已经做过了。

We have httpenv we created earlier.

我们有之前创建的 httpenv。

We have services here where we create the deployment, and

then we create a service with the exact same name.

这里有一些服务，我们在其中创建部署，然后用完全相同的名称创建一个服务。

That's also true of a DaemonSet.

DaemonSet 也是如此。

We can have a deployment with rng and a DaemonSet with rng.

我们可以有一个使用 rng 的 deployment 和使用 rng 的 DaemonSet。

It's okay as long as they're not the same name with

the same resource type.

只要它们不是具有相同资源类型的相同名称就可以。

You'll notice on this line down here, if you're not in a

multi-node setup like I am not.

你会注意到下面这一行，如果你不像我一样在多节点设置中。

I just have the single Docker Desktop locally.

我在本地只有一个 Docker Desktop。

You'll notice that you have a desired here.

你会注意到这里有一个你想要的。

So, this desired current ready.

所以，这个想要的电流准备好了。

These are all new features of the DaemonSet in the get

all kubectl, and that is going to match

the number of nodes you have.

这些都是 get all kubectl 中的 DaemonSet 的新特性，
它将与您拥有的节点数量相匹配。

So, if you have a two-node setup, you'd see two here.

如果你有两个节点的设置，你会在这里看到两个。

If you had a three-node setup, you'd see three here.

如果你有一个三节点的设置，你会在这里看到三个。

The reason for that is that again, it's issuing

one pod per node regardless of how many you have.

原因同样是，不管你有多少个节点，它都会为每个节点发出一个 pod。

Like we saw, we now have too many pods.

就像我们看到的，我们现在有太多的 pod。

We have the ones from deployment and the DaemonSet.

我们有来自 deployment 和 DaemonSet 的。

You can tell the names a little bit because each

nested resource type is going to

create part of the name.

您可以稍微说出一些名称，因为每个嵌套的资源类型都将创建名称的一部分。

So that dash, the way that I know this and to sort

of a little bit inside information, that

the rng first part here is technically

the ReplicaSet that was created.

所以那个 dash，我知道这个还有一些内幕消息，rng 的第一部分在技术上是被创建的副本集。

So, on the original deployment, the deployment

created a ReplicaSet, and the ReplicaSet created a pod, and

that's what created that full name.

Whereas, we don't have ReplicaSets inside

of a DaemonSet. We just need the pod name right

there. Just in case you have a multi-node

setup, or you're using something like kubeadm, or some

other server deployment solution,

it may taint your master nodes.

Those control plane nodes, even on a one-node setup,

depending on the tools you use, may prevent regular

pods from running on those master nodes.

That's also true of DaemonSet.

It will prevent potentially these

DaemonSets from running on the master node, which would

mean that you would have one less if you just had one

master. You'd have one less DaemonSet pod

running in the total count.

And that's normal to keep regular applications off of your

masters in a multi-node setup. We're, for

this little part of the course that we're just on a test

machine, we're using Docker Desktop, or MicroK8s,

or K3s, or MiniKube or all

these other options that are designed for learning, and

testing, and local developer setups.

Those don't typically have the taints, but even

if you had taints, it's ok.

We'll talk about taints and tolerations later.

Tolerations allow you to put pods on a master

even though they have a taint preventing it.

All right. Let's go to look at our webui and see if that

made any difference.

All right. On my machine, I was pretty steady at

10 for the miners.

But, the previous deployment, remember, we deployed

a YAML earlier, and that caused me to

have a pretty steady clip there.

Now, I'm up at 15. So, this definitely did something.

Just even adding one rng through the

DaemonSet, in addition to the deployment, made

a difference. But, I've got two containers now, and

if you're on a single-node setup like me, you've got just

those two containers. So, why did two suddenly

make a difference? If you think about it, we added a

DaemonSet, but the only way this could have possibly

improved is if the service

was using both of the pods.

But, if it's using both of the pods, that means that

somehow our DaemonSet, and our deployment, are

both under the same service.

This is kind of weird. In the next video, we're going to

talk about what just happened and why it may, or may

not, be a good thing for you.

