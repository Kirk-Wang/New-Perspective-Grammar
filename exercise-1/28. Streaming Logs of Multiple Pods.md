Let's go back to that logs command.

让我们回到 logs 命令。

Let's just see a little bit more about dealing with

multiple pods.

我们再看一下如何处理多个 pod。

In fact, let's try a logs command to see

if we can see the pingpong, which you should still

have running from a couple of lectures ago.

事实上，让我们尝试一个日志命令，
看看我们是否可以看到 pingpong，
你应该仍然有运行，来自前两节课之前。

That pingpong now has three replicas.

那个 pingpong 现在有三个副本。

Let's go see if we can get logs from all of them.

我们去看看能不能从他们那里得到日志。

kubectl logs.

We're going to use -l, which means a selector.

我们将使用 -l，这意味着一个选择器。

We haven't talked about selectors yet, but it's a way

to run commands against more than one

object based on labels.

我们还没有讨论选择器，
但它是一种基于标签对多个对象运行命令的方法。
* against 针对

So, we're going to use the -l, and the label we're going to

look for is run=pingpong, because

these containers are based upon the pingpong

deployment. So, we're going to run this

and we're going to add in --tail like before so we

don't get a huge amount of backlogs.

我们将使用 -l，
我们要查找的标签是 run=pingpong，
因为这些容器是基于 pingpong 部署的。
我们要运行这个，我们要加入，像之前那样的 --tail，
这样我们就不会有大量的积压。

Then we're going to use the -f, which is the short for

keep following the logs and stay attached to that stream,

right. What you'll notice here is because I know

our ping command only does one ping per second, you'll

probably notice that these are happening

about three a second, which means that we're probably

seeing all three of the pods.

然后我们要用 -f，
它是保持跟随日志并保持连接到那个流的缩写。
你会注意到的是因为我知道我们的 ping 命令每秒只执行一次 ping，
你可能会注意到这些大约每秒发生三次，
这意味着我们可能会看到所有三个 pod。

Pretty cool, right?

很酷,对吧?

Let's see what happens when we scale this up even more,

though, because there are limits to this command.

不过，让我们看看当我们进一步放大时会发生什么，因为这个命令是有限制的。
* even more 甚至更多
* though conj. 虽然，尽管, adv. 虽然；不过；然而

If we jump back and we scale up to eight,

we'll find out that there is a limit, by default,

to the number of pods we can stream the logs from

at the same time.

如果我们向后跳，并放大到8，
我们会发现，默认情况下，
可以同时从中传输日志的 pod 数量是有限制的。

I'll hit control c there and then kubectl

scale deployment pingpong replicas=8 this

time.

我在那里点击 control+c，然后这次
kubectl scale deployment pingpong replicas=8。

We'll give it a second.

我们等一下。

We can actually do a kubectl get

pods to see if we have eight running.

Yeah. We've got our sleep from before there

from the cron job, but we can see we've got all the

pingpong's there. That's what we care about.

We're going to do kubectl logs -l

again, so it's the same command as before.

run=pingpong.

Tail it with one and then

follow. All right, now we're getting an error this

time saying you tried to follow 8 log streams

and you can only do 5.

This is a setting that has changed recently

in Kubernetes configuration.

This is all based around performance.

In the past, we wouldn't even

have this. You can actually see down at the bottom, we have

max log requests. We can use that to increase the limit.

But why is this the case?

Well, pulling logs is a rather strenuous

thing, especially if your apps are dumping

logs en masse.

If you're ever running a proxy or a web server,

and you're capturing every connection into logs, that's

a large amount of logs streaming over the network.

Even if it's on just one machine, that

can be a lot of work for a computer.

It's pulling this from the API.

So, if you think about how this connection is happening,

you're over here typing your kubectl command.

That is being sent to the

API, running on the control plane.

Then that API needs to get the logs

from the pods.

Imagine if you had three,

different nodes all running different pods.

Of course, in our setup here, we've only got it

on one machine probably.

But, if you had three nodes, that could

be happening and each one of these has the kubelet

agent running on it.

So, the API needs to make one connection

to each kubelet.

Then, that kubelet is going to look

at the pod and the container

logs in that pod.

But wait, there's more.

What's technically happening here is it's not just one

connection. For each of these pods, there

are more connections happening.

For each pod, there's a new connection coming out of

kubectl. Then, the API has

one connection to each kubelet.

So, we've got a lot of back and forth happening here,

and this could easily overwhelm your API.

That's important for us to not do.

We don't want to overwhelm the API.

It's the most precious thing in your cluster.

So, you want to protect that, and there's a natural limit

here. You can change that limit, but there's probably

a better way. While we're here, let's just talk about

other shortcomings of the log command.

I've listed them here.

It's a lot of shortcomings.

There's no date timestamp or information about

which Pod sent that particular log line.

If pods change by coming in and out, you know, being

replaced or deleted, they're not changed

in the command. It's only what was working at the time the

command started.

That command got kind of long, like having to write

all that stuff just to get the particular multi-pod

setup from one deployment was a little bit unnecessary.

Luckily, we have

external third-party tools.

The best one for this that I like right now is Stern.

Let's check that next.

