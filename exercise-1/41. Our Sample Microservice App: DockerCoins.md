Now we

come to the time to learn about the application we'll be using throughout

a large portion of this course.

What's it going to be?

What do you think? Any bets?

Bet you didn't think it was going to be a DockerCoin miner.

That's a new one, isn't it? My friend, Jerome, he

built up this system a long time ago.

Actually, 2015, before Kubernetes was ever invented.

You can't do anything with these coins

other than earn my respect.

But, it's similar in the idea of maybe

something like a Bitcoin miner.

But, we're not doing bitcoins here.

This isn't anything really like a ledger with all of

the actual cryptocurrency stuff in the background.

This is really about spreading out a couple of workloads

into different microservices and then doing

something with it, storing in a database and then showing

some stats about it. This is made up of five,

main parts. Let's go through those one at a time.

Then we'll look at a network diagram, and then we'll get to

it. We start with the rng.

That's the random number generator.

That's its own little microservice. Then we have the

hasher. The hasher does what you

think it does. It hashes the random number.

Then we have the worker, which is doing all

of the orchestration. That's the wrong word there. We shouldn't

use that. Not orchestration.

It's doing all of the handling of the work.

It's handling the different communications.

It's the worker. Then we have the webui, which we

will be staring at on occasion to look at the performance

of our DockerCoin miner. Then we have redis.

That's where we'll be storing the results of our hashing.

Then we'll be using those as a way to check

performance and to make sure that our system is working.

The reason we're doing this type of application, it

might seem kind of unusual, is

because it has two things that are interesting about it. One is that every part of this is

isolated, and technically each one of these is in a

different programming language.

Then the second part of this is that it's always doing

work. It's kind of boring to check an orchestration

system like Kubernetes when your

application isn't doing anything in the background. The

thing about real Bitcoin miners is that they're

constantly doing work. That's the whole point. They're maximizing

resources in IO to try to get as much out

of the system to mine those coins.

But in this case, we're mining these DockerCoins, which

again, are only good for my

admiration and respect if you make a whole lot of them.

But, we're going to be doing that.

It's going to be running in the background and generating

the DockerCoins for us to look at the logs, and the

traffic, and the performance. The way

this thing works in terms of business workflow, if

this was a business, would be the worker reaching

out to the rng microservice, getting back this

random number that the rng generates.

Then it takes that number, sends it off to the harsher.

The hasher hashes it, sends back the

result. Then the worker stores that inside the redis

database. Then it just keeps doing that as fast as it can.

The webui will be for us, the administrator,

to check the performance of our DockerCoin mining.

This is a pretty simple, but effective, diagram

of this application.

We are using all HTTP REST calls here.

For the rng hasher, we're

talking in gets and posts. With redis, we're doing the

redis protocol to send the data to redis.

Then our webui is reaching out to redis to

check how many things

have been mined per second and giving us a nice little

graph of that. You might wonder in

Kubernetes how these things are communicating with each

other. The answer there is that they're communicating the

same way we've known all along.

That's with service names. When you were in Docker

containers back with Docker networks, we

had not necessarily service names, but they were container

names on bridge networks.

Then if you ever played around with Docker Compose or

Swarm, those were services.

We called them that. They all operated on the

names of the services, similar to the name of the container

in regular old Docker run.

Here in Kubernetes, we have services that again, present

us with nice, friendly DNS hostnames.

Those are stored in CoreDNS and

it's delivering up the results like a DNS server should.

In our application source code, which

again is different languages for each part of the app, we

hardcode the names in here.

This is technically bad practice, right?

If you've ever looked back at Twelve-Factor, you know that

in general terms, we're supposed to use environment

variables, or some sort of configuration, to then present

to each app the names for the other parts of the app.

This is just a demo app, of course, so we can't do

all the things at once to show you how Kubernetes works.

The funny thing here is that it doesn't matter where this

app runs.

We were running this app in 2015 in Docker Compose

and Docker run. We were running this in 2016 in

Swarm.

Now here in 2018, 2019 and beyond, we're running it in

Kubernetes, and we haven't changed any of this.

These names have been in the source code.

If you go all the way back through the get log, all the way

back to 2015 before we ever had orchestration.

Because we always had in containers, the idea

that on a single applications network,

at the different parts of the application can find each

other by their friendly service names.

So, I'm going to leave it up to you to decide whether you

ever want to hardcode things in like this.

In here, I generally still use environment

variables. That way I can change the names of things just

in case. But, with the flexibility of Kubernetes, you

might argue that you can create these services and have

them point to different backends whenever you'd like.

So, they don't really have to be the same name and DNS,

but who cares? For now, let's just stick with the names

hardcoded in the application.

That's how the parts find each other.

Which means we're going to have to have services for the

different parts to talk.

We'll all see the UI in a minute once we launch this

application.

It's designed by us backend developers.

It's not a really good frontend. It's crap.

It's not good.

You would not use this as a way to make a pretty graph on a

website. Trust me.

We didn't really make this for that purpose.

We're orchestration administrators right now.

We're playing the role of Kubernetes Ops.

We don't really care about the frontend fanciness or the

accuracy to the milliseconds.

We just want a nice UI to tell us how many per

second, on average, we're mining, and to make

sure that the server's working.