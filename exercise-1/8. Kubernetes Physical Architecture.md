All right. We just had a video about Kubernetes

architecture. I did some scribbling on a screenshot.

We're going to go over that same thing again in a different

way, because it bears repeating that these are the

components that will make up every design,

of every cluster, you build.

So, it's good to talk about them in different ways.

So, here's a list, right.

We have on any single node, you're going to

have a container engine.

In this case, it's Docker.

You could have the different

container engines that Kubernetes supports.

Right now, it's two other ones, cri-o and containerd.

But, it doesn't really matter.

It just needs to be a container engine because Kubernetes

itself doesn't start your containers.

It needs that lower level component provided to it.

Then we have a kubelet agent on each node

that's taking orders and talking back and forth to the API.

Then we have the kube-proxy which receives orders

from the node agent and takes

action on the settings for networking on the local machine.

We'll talk more about that later as well.

Now, you might see in old documentation that these are

referred to as minions, but we're getting away from that.

It's really nodes is the terminology we're using here.

When we talk about the masters, I

like to refer to it as the control plane because it may be

one or more master nodes, and

control plane refers to all those services that are in

control of the cluster.

We're really talking about a core set of services here.

Etcd Is that database.

We've got to have it.

The API server is the in and out point for

all communications. It's how all the tools, and the web

interfaces, and everything communicates to Kubernetes.

Then we have the core services. Things like controller

manager, scheduler, and I mentioned

that CoreDNS. That's also something that you're going to be

seeing a lot of. It's not technically required, but we

might as well consider it required because we're going to

need DNS in just about every cluster we run.

Here's another way of looking at the layout of

how these parts fit together.

It's pretty cool here because the colors will differentiate

the type of protocols.

If you get really into networking protocols and how these

things are communicating, these colors will make a

difference. But, just notice that...realize

we've got technically at the bottom three nodes.

Each one of those has networking components

like kube-proxy, possibly a different, additional

feature depending on how you set the networking.

We'll talk more about that later.

There's always going to be some sort of networking

component there. There's the kubelet agent, then there's

your Docker or other container runtime, and then the OS,

right. Usually Linux, maybe Windows.

Then again, up at the top, we're talking about the

etc database. That's a key value store.

That will be on one or more nodes, depending on whether you

need high availability.

Then you have the controller manager which

manages a bunch of controllers. We'll talk about

controllers later.

You've probably heard of some of them already, like

deployments and ReplicaSets. We'll get more into that.

Then the API server, you can see how everything's talking

through that API server in the middle.

The scheduler over there, on the side, on

the right side, is making decisions about

where things should go on which nodes.

All right. The interesting thing about Kubernetes, and also

may be part of this challenge, is that it's so flexible

in how you can design and deploy it that

you don't even have to run the control plane in containers.

You can run those separately on individual machines,

on the bare host. Or, you can run them all on one machine,

in containers, or a little bit of both.

You might run etcd on its own servers

and separate that database out into its own

replicated set of three or more servers for redundancy.

Then, bring the other components to separate

servers and maybe run them in containers.

It doesn't really matter.

It depends on your deployment tools.

What kind of features those deployment tools have for how

your design is going to lay out. Now, most of us that are

all using small clusters, we just use the defaults

that come with however our distribution wants to install

it. So, if you use something like Docker Enterprise,

it's going to want three servers for redundancy.

We'll talk more about etcd and how three is the minimum

you need for redundancy on your masters.

Then, it will push around the

different components to where it needs them based on their

design for how to make it stable and redundant.

You might see this sometimes called the multi-master setup,

which is what most of us are going to need in production.

We're going to need the HA, high availability

in production.

We are probably going to use different tools, and even

different distributions, for going into production

than we might use on a local machine.

For me, I'm going to be using Docker Desktop a lot because

Kubernetes is built in on my local machine, and it's nice

and easy. But, that doesn't work in production.

I would need a separate distribution, on separate

OS's to be able to install that, especially in a high

availability mode, which for Kubernetes can be lots of

work unless you're using a distribution tool that

will automate all that for you.

If you choose something like a cloud distribution,

an important note here is that part of what they're

offering you is that they're taking care of that

control plane for you. They're running the API and

managing the etc database.

So, in a lot of these cases, you don't even have

a view of that infrastructure and what it looks like.

They just tell you the API endpoint and give you

credentials. That makes it so much easier to

run with Kubernetes. In fact, as a consultant, when I'm

working with clients, I'm usually looking for reasons

to get them into a cloud Kubernetes like Google's

Kubernetes, or Amazon's, or Azure's, or Digital Ocean's.

Because that's going to save them so much more time, and

it's going to let the experts handle the logic

and the decisions around how the control plane runs.

For most of those, the only job you have left is to just

spin up some nodes, however big you want them to be, and

then run a command or two to connect them

to the cluster. Now they become nodes in

that cluster that's provided to you, at least the API,

right. So, it separates the concerns.

You manage your nodes, they manage the control plane.

That's how a lot of them work.