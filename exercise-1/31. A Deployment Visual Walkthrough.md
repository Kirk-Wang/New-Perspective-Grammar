What's really happening when we do that

kubectl run command?

I mean, we run it, and then we do a get all and we see a

bunch of things. But, how is that stuff getting to

the point of a container getting started?

There's actually a lot of steps.

You could write an entire short story around

how this happens, but sometimes visualizations

are a little easier. Let's walk through this, each

action at a time, from the time you type the

command in the CLI, all the way through to

how it goes into a container starting.

Imagine we have this control plane.

We don't know whether it's one master server or a bunch of

them. We just have the proper parts that we need.

We have the etcd, the API.

We have the scheduler, the controller manager.

Then in this case, we're going to have two worker nodes,

right.

They're both running the kubelet and the kube-proxy, maybe

a networking overlay.

But, the point here is these are the minimum components to

get started. Imagine you're on your machine.

It could be remote. It could be on the server itself.

You type a kubectl run command

of web. We're going to call it web.

We're going to use the Nginx image to create

three web servers as three replicas.

When you do that, you're talking with your kubectl

to the API directly.

That's how the command gets sent.

It's talking to the API.

That API server will then store

a new object for this resource, the deployment resource,

based on that command.

It knows that you're essentially asking for

a deployment at that point, and it's storing it in the etcd

database.

And, cool.

You get back a little notification that says we're created.

Job well done.

The difference is, though, is that it's not really done.

This all happened super fast in just a couple of seconds,

but you get returned a result.

That doesn't mean the container is running.

It just means that the object for this resource

was stored in the database successfully.

What happens next?

Well, this controller manager does exactly what it says.

It is a management API that sits

there with a bunch of different controllers for resources,

and it's constantly checking to see what the database

has for it. In this case, it's going to ask the API

server, hey, do you have any new controllers

that I'm not aware of? It will find that there's a

deployment resource available.

Cool. It will then create the resource

that the spec, for that deployment, requests.

That deployment is saying, I would like a ReplicaSet, and

here is the spec for the ReplicaSet.

The controller manager does that.

It writes another object record to the etcd database,

and then that transaction is over.

Then we wait, maybe a few microseconds,

maybe a few nanoseconds.

Then, the controller manager is requesting again.

Then we wait.

This is actually the controller manager polling interval

that you can set up. If you get really into production

clusters, you can change that, but it's less than a second.

It's going to check again, and this time, it will notice a

new resource object.

All of the ReplicaSet.

Cool. It now has more work to do, so it will

then say, OK. This ReplicaSet is requesting three

pods with this spec.

So, I will create three, identical

pod specs inside the etcd database

with a status of pending.

Then it's done.

On the scheduler side of things, the scheduler also

pulls the API every so often.

This is hitting the API server and saying,

are there any new pods that I need

to schedule somewhere in this cluster?

It finds three pods pending, which indicates

that they are not yet scheduled for a particular node.

So, the scheduler runs its algorithm and does a bunch of

analysis of which servers are most available, and

if there's resources to do this thing, and meets all the

requirements, essentially, including stuff like taints that

we'll learn about later.

But, none of that stuff really pertains to the situation

because we're not setting any limits.

We're not setting any label requirements

or any taints. So, it's just going to schedule those across

the worker nodes as much as possible.

So, it's going to assign one to node 2, and two

to node 1, and spread them out.

But, it's not the scheduler's job to put them

on the server. Its role is to decide where they

go and then to change their configuration and etcd

for where they should go. Are you starting to see how these

different microservice parts work together?

Now, the kubelet is up to bat.

The kubelet is in communication with the API on a regular

basis, and it's looking for work to do.

In this case, it's saying, are there any pods that are

assigned to my node that I am not yet running?

In this case, they both find that yes, indeed.

There are pods waiting to be run that they

don't have listed as running containers on their node.

So, they will change their status in

the database to creating.

Then, they will go and tell the runtime,

in this case Docker, to create a container

and launch the application.

Once the application has successfully started, it will then

update the etcd database to change the status to

running so that anything else needing to know

will be aware that these pods are now running.

If you want to run through this just one more time

to just really nail this down and get this

workflow in your head, this is important.

Because this helps you understand the key differences

of the different parts, right.

The scheduler doesn't actually place things anywhere.

It's simply looking at algorithms, looking

at what's really going on in the nodes and figuring out

where things can run. Then it's just updating etcd.

A lot of these parts are doing a little bit of work and

then updating the records in etcd. The kubelet

is really the one doing the work on each node.

It's creating the containers, running the healthchecks,

making sure the containers are matching what the pod

assignments are in the database.

Now you can see that there is a lot going on the background

even after our command returns that it's

completed.

