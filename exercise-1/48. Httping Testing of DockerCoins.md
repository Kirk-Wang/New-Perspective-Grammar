Now that we know we've hit a bottleneck with the worker,

we have to look at the other parts of this app to see if

they're the bottleneck. If you had better instrumentation

in your apps, which I hope you do, because ours has none.

You would have cool applications like the ones listed here,

or other projects, that would give you more data about the

metrics from each part of your app.

That's really important in distributed apps because you

have them all spread over the place.

So, the network is part of the app.

Which means you to start tracking network performance

of the different parts more importantly. You can read that

all over the Internet about microservices and the

challenges of monitoring and troubleshooting microservices.

We can use a couple of simple utilities here to get basic

performance out of this app.

One of those is a httping.

That's a cool utility that I like to use.

It's the combination of ping for HTTP.

That's really all it is.

We're going to run it from the command line just like we

would a ping utility, but it's going to use the

HTTP protocol to connect to these applications.

We can't use it on the worker because that doesn't have an

endpoint. But, we can use it on rng and the hasher.

First up, let's get back to our command line.

We're going to take the cluster IP,

and we could just copy and paste them, but we're going to

use the same bash one liner here that sets an

environment variable to the IP address of each services

cluster IP.

So, instead of typing these out, I'm just going

to copy and paste them. We've done this command before,

and we're going to use those to save us some time.

That way, we can use these different

hasher and rng IP addresses in various commands.

Then, we're going to use the shpod utility

to get into our cluster since these things don't have node

ports. We need to be in the cluster unless you're on

a Linux host where you have native access to these IP

addresses. The first two commands here, if you're on Mac

or Windows with Docker Desktop, or other tools,

you can do this, kubectl apply like we showed earlier.

Then you can also attach to that so that we get a

shell in our cluster.

This shell will also have a httping installed.

You can get httping on Linux like with APT,

get it on a Mac with Brew.

The last I looked you couldn't get it natively on Windows,

but that's why we have this shpod.

All right. I ran the kubectl apply to create my

pod, or sh-tools.

Now I'm in it. It should look like this to you as well,

where you have a command line. If you don't have that

colorful command line, just hit enter and you should see

it. We're going to run httping here,

and we're going to use the environment variables, but

you're going to need to get those environment variables in

to the shpod if you need that.

You might need to go back a slide now that you're in shpod.

Copy those two lines out, run them again inside

that pod.

Then, we're going to go back to those httping commands.

All right. First command is httping -c

3 with

the hasher.

Of course, your IP address is going to be different than

mine. You can see here it just did three

pings to the HTTP protocol.

You can see that the important part I'm looking at here is

the response time.

We've got six milliseconds.

That's pretty pretty standard for a small little web

service. That's pretty quick.

Nothing to complain about there.

Let's look at the rng.

We have quite a different return

value. Look at those milliseconds. That's almost a full

second in response time.

That definitely seems to be suspect.

I know what's wrong with this app.

You don't know that yet, but obviously we put something in

here that's deliberate.

This isn't a true troubleshooting scenario.

It is a hasty conclusion here.

That hasty conclusion is well, the rng seems to

be the bottleneck because we expect it to be faster than

that. Maybe because a random number

generator generates random numbers, we need to

use multiple servers because we're using random

number generators. On Linux, there's this thing called

dev/random or dev/urandom down at the bottom.

You can geek out on the Internet if you're all interested

in random numbers and how Linux makes them.

We can make up an imaginary story here that I'm going to

say is the reason this is slow is because we don't

have enough entropy on our machine.

We're going to need to scale this out.

Because we're learning Kubernetes and this is just the

beginning, we only have a single-server setup.

If you've gone ahead and deployed a multi-server setup,

then good for you. This next section will make a little

more sense. What we're going to do right now is we're

going to use a new resource type in Kubernetes

that is designed to scale out across multiple

nodes. We won't be able to do that on a single-node

cluster. It'll work. It just won't scale to many different

nodes.

I will explain it along the way.

Trust me, it's going to make sense.

But later on in the course, we'll have the opportunity to

use the Cloud, or some other resources you have, to make a

multi-node setup. Let's get started on the next type of

resource, DaemonSets.